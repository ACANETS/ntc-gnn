{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from graph_nets import blocks\n",
    "from graph_nets import graphs\n",
    "from graph_nets import modules\n",
    "from graph_nets import utils_np\n",
    "from graph_nets import utils_tf\n",
    "import models\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import scipy.io\n",
    "import torch.utils.data as Data\n",
    "import glob\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(data_path):\n",
    "    mat = scipy.io.loadmat(data_path)\n",
    "    x = np.squeeze(mat['flow']/255.0)\n",
    "    y = np.squeeze(mat['label'])\n",
    "    z = np.squeeze(mat['global_att'])\n",
    "    w = np.squeeze(mat['timestamp'])\n",
    "    \n",
    "    return x, y, z, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***adjust variables according to your case\n",
    "\n",
    "datapath\n",
    "- where you store the dataset\n",
    "\n",
    "meta\n",
    "- number of meta features\n",
    "\n",
    "samplesize\n",
    "- total sample size\n",
    "\n",
    "rate\n",
    "- define size of training set\n",
    "\n",
    "folders\n",
    "- folder number under your datapath\n",
    "- depend on how you structured/preprocessed your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "datapath = \"datapath\"\n",
    "meta = 7\n",
    "samplesize = 13341\n",
    "data2 = np.zeros((meta,samplesize))\n",
    "\n",
    "rate = 0.6\n",
    "folders = 18\n",
    "count = 0\n",
    "for i in range(folders):\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    path = datapath + str(i) + '/*.mat'\n",
    "    samples = glob.glob(path)\n",
    "    sample_count = math.ceil(len(samples)*rate)\n",
    "    \n",
    "    for j, data in enumerate(samples):\n",
    "        node, gt, gb, timestamp = data_loader(data)\n",
    "        data2[:,count] = gb[:]\n",
    "        count = count + 1\n",
    "        \n",
    "max_v = np.amax(data2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "Training size:  8013\n",
      "Validation size 5328\n"
     ]
    }
   ],
   "source": [
    "training = []\n",
    "ground_truth = []\n",
    "\n",
    "validation = []\n",
    "val_ground_truth = []\n",
    "\n",
    "for i in range(folders):\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    path = datapath + str(i) + '/*.mat'\n",
    "    samples = glob.glob(path)\n",
    "    sample_count = math.ceil(len(samples)*rate)\n",
    "    \n",
    "    for j, data in enumerate(samples):\n",
    "        node, gt, gb, timestamp = data_loader(data)\n",
    "        \n",
    "        # put meta features of a flow to global\n",
    "        gb[0] = gb[0]/max_v[0]\n",
    "        gb[1] = gb[1]/max_v[1]\n",
    "        gb[2] = gb[2]/max_v[2]\n",
    "        gb[3] = gb[3]/max_v[3]\n",
    "        gb[4] = gb[4]/max_v[4]\n",
    "        gb[5] = gb[5]/max_v[5]\n",
    "        gb[6] = gb[6]/max_v[6]\n",
    "\n",
    "        # put timestamps of a flow to edges\n",
    "        if len(node.shape) == 2:\n",
    " \n",
    "            if node.shape[1] >1500:\n",
    "                print(node.shape)\n",
    "\n",
    "            node_count = node.shape[0]\n",
    "            edge = []\n",
    "            sender = []\n",
    "            receiver = []\n",
    "            \n",
    "            for k in range(node_count):\n",
    "                for m in range(k+1, node_count):\n",
    "                    \n",
    "                    time_ = timestamp[m]-timestamp[k]\n",
    "                    edge.append([time_.astype(np.float32)])\n",
    "                    sender.append(k)\n",
    "                    receiver.append(m)\n",
    "            \n",
    "        elif len(node.shape) == 1:\n",
    "            print(\"only 1 node, something's wrong!!\")\n",
    "\n",
    "        else:\n",
    "            print(\"something's wrong!!\")\n",
    "\n",
    "        # put packets of a flow to nodes\n",
    "        # use first 1500 bytes of each packets\n",
    "        node = node[:,:100]\n",
    "\n",
    "        globals_0 = gb.tolist()\n",
    "\n",
    "        data_dict_0 = {\n",
    "            \"globals\": globals_0,\n",
    "            \"nodes\": node.tolist(),\n",
    "            \"edges\": edge,\n",
    "            \"senders\": sender,\n",
    "            \"receivers\": receiver\n",
    "        }\n",
    "        data_dict_1 = {\n",
    "            \"globals\": gt\n",
    "        }\n",
    "        \n",
    "        if j<sample_count:\n",
    "            training.append(data_dict_0)\n",
    "            ground_truth.append(data_dict_1)\n",
    "        else:\n",
    "            validation.append(data_dict_0)\n",
    "            val_ground_truth.append(data_dict_1)\n",
    "            \n",
    "print(\"Training size: \", len(training))\n",
    "print(\"Validation size\", len(validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***adjust variables according to your case\n",
    "\n",
    "cls\n",
    "- classes to classify\n",
    "\n",
    "***you can modify EncodeProcessDecode() in model.py according to your case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "koko-build()\n",
      "EncodeProcessDecode(global_output_size=6)\n"
     ]
    }
   ],
   "source": [
    "# Full Graph Network Block\n",
    "cls = 6\n",
    "OUTPUT_SIZE=cls\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "\n",
    "    net = models.EncodeProcessDecode(               \n",
    "        edge_output_size=None,\n",
    "        node_output_size=None,\n",
    "        global_output_size=OUTPUT_SIZE,)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class name tag\n",
    "cls_names = [\"chat\", \"email\", \"file\", \"streaming\", \"torrent\", \"voip\"]\n",
    "\n",
    "def create_loss_ops(output, target):\n",
    "    # calculate weight according to sample size of each class\n",
    "    weights = tf.constant([1.0/0.3019, 1.0/0.0223, 1.0/0.0709, 1.0/0.0441, 1.0/0.0337, 1.0/0.5268])\n",
    "    \n",
    "    # with weights\n",
    "    loss = tf.compat.v1.losses.softmax_cross_entropy(onehot_labels=weights*tf.one_hot(tf.cast(target.globals, tf.int32), cls)\n",
    "                                                     , logits=output.globals)\n",
    "    # without weights\n",
    "    #loss = tf.compat.v1.losses.softmax_cross_entropy(onehot_labels=tf.one_hot(tf.cast(target.globals, tf.int32), cls)\n",
    "    #                                                 , logits=output.globals)\n",
    "    return loss\n",
    "\n",
    "def update_step(inputs_tr, targets_tr):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs_tr = net(inputs_tr, is_training=True)\n",
    "        # Loss.\n",
    "        loss_tr = create_loss_ops(outputs_tr, targets_tr)\n",
    "\n",
    "    gradients = tape.gradient(loss_tr, net.trainable_variables)\n",
    "    optimizer.apply(gradients, net.trainable_variables)\n",
    "    \n",
    "    return outputs_tr, loss_tr\n",
    "\n",
    "# define EPOH, batch size for training & validation\n",
    "EPOCH = 300\n",
    "BATCH_SIZE = 128\n",
    "BATCH_SIZE_V = 350\n",
    "\n",
    "max_acc = 0.0\n",
    "max_acc_epoch = 0\n",
    "\n",
    "# set learning rate\n",
    "learning_rate = 3.5e-4\n",
    "optimizer = snt.optimizers.Adam(learning_rate)\n",
    "\n",
    "for i in range(EPOCH):\n",
    "    # shuffle\n",
    "    temp = list(zip(training, ground_truth))\n",
    "    random.shuffle(temp)\n",
    "    train_2, gt_2 = zip(*temp)\n",
    "    temp_loss = 0.0\n",
    "    \n",
    "    print(\"EPOCH:\", i)\n",
    "    \n",
    "    for k in range(math.floor(len(train_2)/BATCH_SIZE)):\n",
    "    \n",
    "        with tf.device('/GPU:0'):\n",
    "            # get input data\n",
    "            inputs_graph = utils_tf.data_dicts_to_graphs_tuple(train_2[k*BATCH_SIZE:(k+1)*BATCH_SIZE])\n",
    "            targets_graph = utils_tf.data_dicts_to_graphs_tuple(gt_2[k*BATCH_SIZE:(k+1)*BATCH_SIZE])\n",
    "            with tf.GradientTape() as tape:\n",
    "                outputs_tr = net(inputs_graph, 1)\n",
    "                # Loss.\n",
    "                loss_tr = create_loss_ops(outputs_tr[0], targets_graph)\n",
    "            \n",
    "            temp_loss += loss_tr\n",
    "            gradients = tape.gradient(loss_tr, net.trainable_variables)\n",
    "            optimizer.apply(gradients, net.trainable_variables)\n",
    "    \n",
    "    total2 = np.zeros(cls)\n",
    "    cf = np.zeros((cls, cls))\n",
    "    \n",
    "    acc = tf.keras.metrics.Accuracy()\n",
    "    acc.reset_states()\n",
    "    \n",
    "    # validation acc\n",
    "    ranc = math.ceil(len(validation)/BATCH_SIZE_V)\n",
    "    ranf = math.floor(len(validation)/BATCH_SIZE_V)\n",
    "    last_bit = len(validation) - ranf*BATCH_SIZE_V\n",
    "    for k in range(ranc):\n",
    "\n",
    "        with tf.device('/GPU:0'):\n",
    "\n",
    "            if k <= ranc - 2:\n",
    "                inputs_graph = utils_tf.data_dicts_to_graphs_tuple(validation[k*BATCH_SIZE_V:(k+1)*BATCH_SIZE_V])\n",
    "                targets_graph = utils_tf.data_dicts_to_graphs_tuple(val_ground_truth[k*BATCH_SIZE_V:(k+1)*BATCH_SIZE_V])\n",
    "            else: \n",
    "                inputs_graph = utils_tf.data_dicts_to_graphs_tuple(validation[k*BATCH_SIZE_V:])\n",
    "                targets_graph = utils_tf.data_dicts_to_graphs_tuple(val_ground_truth[k*BATCH_SIZE_V:])\n",
    "            outputs_tr = net(inputs_graph, 1)\n",
    "            \n",
    "        pred = tf.math.argmax(outputs_tr[0].globals, 1)\n",
    "        _ = acc.update_state(targets_graph.globals, pred)\n",
    "            \n",
    "        if k <= ranc - 2:   \n",
    "            for j in range(BATCH_SIZE_V):\n",
    "                total2[val_ground_truth[j+k*BATCH_SIZE_V]['globals']] += 1\n",
    "                cf[val_ground_truth[j+k*BATCH_SIZE_V]['globals'], pred[j]] += 1\n",
    "        else:\n",
    "    \n",
    "            for j in range(last_bit):\n",
    "                total2[val_ground_truth[j+k*BATCH_SIZE_V]['globals']] += 1\n",
    "                cf[val_ground_truth[j+k*BATCH_SIZE_V]['globals'], pred[j]] += 1\n",
    "    if acc.result().numpy() > max_acc:\n",
    "        max_acc = acc.result().numpy()\n",
    "        max_acc_epoch = i\n",
    "\n",
    "    print(\"Max ACC: \", max_acc, \"   / Max epoch: \", max_acc_epoch)\n",
    "    print(\"Val ACC: \", acc.result().numpy())\n",
    "    \n",
    "    \n",
    "    for yy in range(cls):\n",
    "        print(cls_names[yy], \": \", int(total2[yy]), end=\" \")\n",
    "\n",
    "    print(\"\")\n",
    "    for yy in range(cls):\n",
    "        print(cls_names[yy], \" acc: \", cf[yy,yy]/total2[yy], end=\" \")\n",
    "    print(\"\")    \n",
    "    \n",
    "    np.set_printoptions(precision=3,suppress=True)\n",
    "    print(cf)\n",
    "    \n",
    "    total3 = np.zeros(cls)\n",
    "    cf3 = np.zeros((cls, cls))\n",
    "    \n",
    "    acc.reset_states()\n",
    "    \n",
    "    ranc = math.ceil(len(training)/BATCH_SIZE_V)\n",
    "    ranf = math.floor(len(training)/BATCH_SIZE_V)\n",
    "    last_bit = len(training) - ranf*BATCH_SIZE_V\n",
    "    \n",
    "    # check training acc\n",
    "    for k in range(ranc):\n",
    "            \n",
    "        with tf.device('/GPU:0'):\n",
    "            if k <= ranc - 2:\n",
    "                inputs_graph = utils_tf.data_dicts_to_graphs_tuple(training[k*BATCH_SIZE_V:(k+1)*BATCH_SIZE_V])\n",
    "                targets_graph = utils_tf.data_dicts_to_graphs_tuple(ground_truth[k*BATCH_SIZE_V:(k+1)*BATCH_SIZE_V])\n",
    "            else:\n",
    "                inputs_graph = utils_tf.data_dicts_to_graphs_tuple(training[k*BATCH_SIZE_V:])\n",
    "                targets_graph = utils_tf.data_dicts_to_graphs_tuple(ground_truth[k*BATCH_SIZE_V:])\n",
    "                \n",
    "            outputs_tr = net(inputs_graph, 1)\n",
    "        pred = tf.math.argmax(outputs_tr[0].globals, 1)\n",
    "        _ = acc.update_state(targets_graph.globals, pred)\n",
    "        if k <= ranc - 2:\n",
    "            for j in range(BATCH_SIZE_V):\n",
    "                total3[ground_truth[j+k*BATCH_SIZE_V]['globals']] += 1\n",
    "                cf3[ground_truth[j+k*BATCH_SIZE_V]['globals'], pred[j]] += 1\n",
    "        else:\n",
    "            for j in range(last_bit):\n",
    "                total3[ground_truth[j+k*BATCH_SIZE_V]['globals']] += 1\n",
    "                cf3[ground_truth[j+k*BATCH_SIZE_V]['globals'], pred[j]] += 1\n",
    "            \n",
    "    print(\"Training ACC: \", acc.result().numpy())\n",
    "    \n",
    "    for yy in range(cls):\n",
    "        print(cls_names[yy], \": \", int(total3[yy]), end=\" \")\n",
    "    print(\"\")\n",
    "    for yy in range(cls):\n",
    "        print(cls_names[yy], \" acc: \", cf3[yy,yy]/total3[yy], end=\" \")\n",
    "    print(\"\")  \n",
    "    my_plot[i, 1] = acc.result().numpy()\n",
    "\n",
    "    np.set_printoptions(precision=3,suppress=True)\n",
    "    print(cf3)\n",
    "    \n",
    "    print(\"\\n\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
